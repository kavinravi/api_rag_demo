{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# rag demo - openai + langchain\n",
        "\n",
        "need a `.env` file with:\n",
        "```\n",
        "OPENAI_API_KEY=your_key\n",
        "MODEL_NAME=\"gpt-4o-mini\" (you probably don't want a reasoning model, this one is good & fast & cheapish)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# setup\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_community.document_loaders import DirectoryLoader, TextLoader\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_chroma import Chroma\n",
        "\n",
        "load_dotenv()\n",
        "MODEL = os.getenv(\"MODEL_NAME\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loaded 5 docs\n"
          ]
        }
      ],
      "source": [
        "# load the txt files\n",
        "loader = DirectoryLoader(\"./rag_data\", glob=\"**/*.txt\", loader_cls=TextLoader)\n",
        "docs = loader.load()\n",
        "print(f\"loaded {len(docs)} docs\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# chunk docs \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "14 chunks\n"
          ]
        }
      ],
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(f\"{len(chunks)} chunks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/44/_9r3lfrd4h1bt3rsqqzzs7sm0000gn/T/ipykernel_76861/3701351219.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the `langchain-huggingface package and should be used instead. To use it run `pip install -U `langchain-huggingface` and import as `from `langchain_huggingface import HuggingFaceEmbeddings``.\n",
            "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vectorstore ready\n"
          ]
        }
      ],
      "source": [
        "# embed and store in chroma (local model, no api limits)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
        "vectorstore = Chroma.from_documents(chunks, embeddings, persist_directory=\"./chroma_db\")\n",
        "print(\"vectorstore ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# setup llm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "using gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "llm = ChatOpenAI(model=MODEL, temperature=0.3)\n",
        "print(f\"using {MODEL}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# helper to ask questions (rag only)\n",
        "def ask(q):\n",
        "    docs = vectorstore.similarity_search(q, k=3)\n",
        "    context = \"\\n\\n\".join([f\"[{d.metadata['source']}]\\n{d.page_content}\" for d in docs])\n",
        "    prompt = f\"answer based on this context, cite which source file you found the info in:\\n{context}\\n\\nquestion: {q}\"\n",
        "    return llm.invoke(prompt).content\n",
        "\n",
        "# ask with an additional input document (put .txt files in ./input/)\n",
        "def ask_with_doc(q, filename):\n",
        "    # read the input doc\n",
        "    with open(f\"./input/{filename}\", \"r\") as f:\n",
        "        input_doc = f.read()\n",
        "    \n",
        "    # get rag context\n",
        "    docs = vectorstore.similarity_search(q, k=3)\n",
        "    rag_context = \"\\n\\n\".join([f\"[{d.metadata['source']}]\\n{d.page_content}\" for d in docs])\n",
        "    \n",
        "    # combine both\n",
        "    prompt = f\"\"\"answer based on this context, cite which source you found the info in:\n",
        "\n",
        "[INPUT: {filename}]\n",
        "{input_doc}\n",
        "\n",
        "{rag_context}\n",
        "\n",
        "question: {q}\"\"\"\n",
        "    return llm.invoke(prompt).content\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ask something\n",
        "structure: print(ask(\"question\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TechVentures holds the following security certifications:\n",
            "\n",
            "- SOC 2 Type II certified\n",
            "- GDPR compliant\n",
            "- HIPAA compliant (Enterprise healthcare module)\n",
            "- ISO 27001 certified\n",
            "- PCI DSS Level 1 (for payment processing features)\n",
            "\n",
            "(Source: [rag_data/example5.txt])\n"
          ]
        }
      ],
      "source": [
        "print(ask(\"what security certs do they have?\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ask with input doc\n",
        "put a .txt file in ./input/ then use ask_with_doc(question, filename)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# example: compare a proposal against existing company policies\n",
        "print(ask_with_doc(\"how does this proposal compare to our current policies? what would change?\", \"sample_input.txt\"))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
